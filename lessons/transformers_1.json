{
  "title": "Transformers: De la Motivación Matemática a la Implementación",
  "steps": [
    {
      "type": "text",
      "content": "<h3>🔍 Crisis de los Modelos Secuenciales (2017)</h3><p>Problemas fundamentales en NLP pre-Transformers:</p><table class='model-limits'><tr><th></th><th>RNN/LSTM</th><th>CNN</th><th>Transformers</th></tr><tr><td>Long-range dependencies</td><td>🚫 O(n) pasos</td><td>🚫 Ventana fija</td><td>✅ O(1)</td></tr><tr><td>Paralelización</td><td>🚫 Secuencial</td><td>✅ Parcial</td><td>✅ Completa</td></tr><tr><td>Coste computacional</td><td>O(n)</td><td>O(k⋅n)</td><td>O(n²)</td></tr></table><p><strong>Insight clave:</strong> La atención elimina cuellos de botella en el flujo de información a través del tiempo.</p>"
    },
    {
      "type": "problem",
      "question": "¿Por qué las RNNs fallan en capturar dependencias a muy largo plazo?",
      "options": [
        "Por el problema de vanishing gradients: ∂L/∂h_t ≈ ∏_{i=1}^t W_{rec} → 0",
        "Debido a limitaciones de memoria hardware",
        "Porque procesan la secuencia al revés"
      ],
      "answer": "Por el problema de vanishing gradients: ∂L/∂h_t ≈ ∏_{i=1}^t W_{rec} → 0",
      "explanation": "📉 <strong>Análisis Matemático:</strong> Si |λ_max(W_{rec})| < 1, el gradiente decae exponencialmente: ||∂h_t/∂h_1|| ~ |λ|^t → 0 para t grande."
    },
    {
      "type": "text",
      "content": "<h3>🧮 La Ecuación Fundamental de la Atención</h3><p>La auto-atención se define como:</p><div class='equation'>\\[ \\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V \\]</div><p><strong>Desglose Matemático:</strong></p><ol><li><em>Producto QKᵀ:</em> Mide compatibilidad entre consultas y claves<br>\\[ \\text{Similitud}(q_i,k_j) = q_i \\cdot k_j \\]</li><li><em>Escalado por 1/√d_k:</em><br>\\[ \\frac{QK^\\top}{\\sqrt{d_k}} \\quad \\text{(Control de varianza)} \\]<p>Si \\( q_i,k_j \\sim \\mathcal{N}(0,1) \\), entonces \\( \\text{Var}(q_i \\cdot k_j) = d_k \\).<br>El escalado mantiene \\( \\text{Var} = 1 \\), evitando saturación del softmax.</p></li><li><em>Softmax:</em> Normaliza a distribución de probabilidad<br>\\[ \\alpha_{ij} = \\frac{e^{s_{ij}}}{\\sum_k e^{s_{ik}}} \\]</li><li><em>Combinación Lineal:</em> Promedia valores según importancia<br>\\[ h_i = \\sum_j \\alpha_{ij}v_j \\]</li></ol>"
    },
    {
      "type": "problem",
      "question": "¿Qué pasaría si elimináramos el factor de escalado 1/√d_k?",
      "options": [
        "Los logits crecerían con d_k → softmax saturado → gradientes pequeños",
        "Mejoraría la eficiencia computacional",
        "No afectaría el rendimiento"
      ],
      "answer": "Los logits crecerían con d_k → softmax saturado → gradientes pequeños",
      "explanation": "📚 <strong>Demostración:</strong> Para d_k=64, E[QKᵀ] = 64 → softmax([64, 64,...]) ≈ [1/n, ..., 1/n] (gradientes ≈ 0)"
    },
    {
      "type": "text",
      "content": "<h3>🎭 Multi-Head Attention: Por qué múltiples cabezas?</h3><p>Definición:</p>\\[ \\text{MultiHead}(Q,K,V) = \\text{Concat}(head_1,...,head_h)W^O \\]<p>Donde cada cabeza:</p>\\[ head_i = \\text{Attention}(QW_i^Q,KW_i^K,VW_i^V) \\]<p><strong>Propiedades Clave:</strong></p><ul><li>🔄 Cada cabeza proyecta en distintos subespacios</li><li>🧠 Captura diferentes relaciones sintácticas/semánticas</li><li>⚡ Aumenta capacidad sin incrementar demasiado cómputo</li></ul>"
    },
    {
      "type": "problem",
      "question": "Si usáramos solo 1 cabeza de atención con dimensión d=512 vs 8 cabezas de d=64:",
      "options": [
        "8 cabezas: Mayor capacidad de representación + paralelismo",
        "1 cabeza: Más eficiente y mejor rendimiento",
        "Es indiferente, son equivalentes"
      ],
      "answer": "8 cabezas: Mayor capacidad de representación + paralelismo",
      "explanation": "🎯 <strong>Análisis:</strong> 8 cabezas permiten:\\n- Paralelización total\\n- Especialización en distintos tipos de patrones\\n- Dimensión total igual (8×64=512)"
    },
    {
      "type": "text",
      "content": "<h3>📍 Positional Encoding: La Geometría del Orden</h3><p>Solución a la permutación-equivariancia de la atención pura:</p>\\[ PE(pos,2i) = \\sin\\left(\\frac{pos}{10000^{2i/d}}\\right) \\]\\[ PE(pos,2i+1) = \\cos\\left(\\frac{pos}{10000^{2i/d}}\\right) \\]<p><strong>Propiedades Matemáticas:</strong></p><ul><li>📐 Representa posiciones relativas mediante combinaciones lineales</li><li>🌀 Frecuencias exponencialmente decrecientes capturan escalas múltiples</li><li>🧩 Permite extrapolación a longitudes no vistas</li></ul>"
    },
    {
      "type": "problem",
      "question": "¿Por qué usar funciones sinusoidales en lugar de embeddings aprendidos?",
      "options": [
        "Para generalizar a secuencias más largas que las de entrenamiento",
        "Porque son más fáciles de optimizar",
        "Para reducir el coste computacional"
      ],
      "answer": "Para generalizar a secuencias más largas que las de entrenamiento",
      "explanation": "📈 <strong>Ventaja:</strong> Los sinusoides codifican posiciones de forma absoluta y relativa simultáneamente mediante identidades trigonométricas."
    }
  ]
}
