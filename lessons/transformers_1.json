{
  "title": "De RNNs a Transformers: La RevoluciÃ³n de la Auto-AtenciÃ³n",
  "steps": [
    {
      "type": "text",
      "content": "<h3>ğŸ” LÃ­mites de Modelos ClÃ¡sicos</h3><table><tr><th>Modelo</th><th>ğŸ”´ Problema Fundamental</th><th>ğŸ“‰ LimitaciÃ³n MatemÃ¡tica</th></tr><tr><td>RNN</td><td>PÃ©rdida de informaciÃ³n en secuencias largas</td><td>Vanishing gradients en âˆ‚L/âˆ‚h<sub>t</sub> (|Î»<sub>W</sub>| < 1 en celdas)</td></tr><tr><td>CNN</td><td>RecepciÃ³n limitada del contexto</td><td>Kernel size fijo â†’ O(n/k) interacciones remotas</td></tr></table><p>â“ <em>ReflexiÃ³n:</em> Si una palabra clave estÃ¡ en la posiciÃ³n 100 de un texto, Â¿cÃ³mo afectarÃ­a esto a una RNN estÃ¡ndar?</p>"
    },
    {
      "type": "problem",
      "question": "Para traducir un texto de 500 palabras donde la estructura gramatical depende de palabras separadas por 200 posiciones, Â¿quÃ© arquitectura serÃ­a mÃ¡s eficiente?",
      "options": [
        "RNN: Procesamiento secuencial con memoria oculta",
        "CNN: Filtros convolutivos locales repetidos",
        "Transformer: Mecanismo de atenciÃ³n completo"
      ],
      "answer": "Transformer: Mecanismo de atenciÃ³n completo",
      "explanation": "âœ… Los Transformers calculan dependencias entre todas las posiciones en O(1) operaciones, evitando la degradaciÃ³n exponencial de gradientes en RNNs."
    },
    {
      "type": "text",
      "content": "<h3>ğŸ§® GÃ©nesis MatemÃ¡tica de la Auto-AtenciÃ³n</h3><p>La ecuaciÃ³n fundamental:</p><p>\\[ \\text{AtenciÃ³n}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V \\]</p><p><strong>DiseÃ±o Intencional:</strong></p><ul><li>ğŸ”‘ <em>Queries/Keys/Values:</em> Analogon a sistemas de recuperaciÃ³n de informaciÃ³n (Q = consulta, K = indice, V = contenido)</li><li>âš–ï¸ <em>Softmax + Escalado:</em> Normaliza influencias y controla la varianza (evita que dot products crezcan con âˆšd<sub>k</sub>)</li><li>ğŸ§© <em>PermutaciÃ³n-Equivariancia:</em> AtenciÃ³n es invariante al orden de entrada â†’ Requiere positional encoding</li></ul><p>â“ <em>Profundizando:</em> Â¿Por quÃ© el producto punto QK<sup>T</sup> y no otra medida de similitud?</p>"
    },
    {
      "type": "problem",
      "question": "Â¿Por quÃ© se escala por 1/âˆšd_k en la atenciÃ³n?",
      "options": [
        "Para mantener varianza â‰ˆ1 y estabilizar entrenamiento",
        "Para reducir la sensibilidad a dimensiones altas",
        "Ambas razones anteriores"
      ],
      "answer": "Ambas razones anteriores",
      "explanation": "ğŸ“ <strong>Teorema:</strong> Si q,k âˆ¼ N(0,1), Var(qÂ·k) = d_k. Escalar por 1/âˆšd_k hace Var = 1 â†’ Evita softmax saturado y mejora flujo de gradientes."
    },
    {
      "type": "text",
      "content": "<h3>ğŸ¯ ConclusiÃ³n: Por quÃ© Funciona</h3><p>Los Transformers superan limitaciones previas mediante:</p><ol><li><strong>Acceso Directo MatemÃ¡tico:</strong> Atienden cualquier posiciÃ³n sin path length (vs O(n) en RNNs)</li><li><strong>ParalelizaciÃ³n Ã“ptima:</strong> Operaciones matriciales en lugar de secuenciales</li><li><strong>Dinamismo Contextual:</strong> Ponderaciones de atenciÃ³n aprendidas, no fijas como en CNNs</li></ol><p>ğŸ”¬ <em>Insight Clave:</em> La atenciÃ³n como aproximaciÃ³n diferenciable de un sistema de memoria asociativa key-value.</p>"
    }
  ]
}
