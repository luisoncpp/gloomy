{
  "title": "Transformers: De la MotivaciÃ³n MatemÃ¡tica a la ImplementaciÃ³n",
  "steps": [
    {
      "type": "text",
      "content": "<h3>ğŸ” Crisis de los Modelos Secuenciales (2017)</h3><p>Problemas fundamentales en NLP pre-Transformers:</p><table class='model-limits'><tr><th></th><th>RNN/LSTM</th><th>CNN</th><th>Transformers</th></tr><tr><td>Long-range dependencies</td><td>ğŸš« O(n) pasos</td><td>ğŸš« Ventana fija</td><td>âœ… O(1)</td></tr><tr><td>ParalelizaciÃ³n</td><td>ğŸš« Secuencial</td><td>âœ… Parcial</td><td>âœ… Completa</td></tr><tr><td>Coste computacional</td><td>O(n)</td><td>O(kâ‹…n)</td><td>O(nÂ²)</td></tr></table><p><strong>Insight clave:</strong> La atenciÃ³n elimina cuellos de botella en el flujo de informaciÃ³n a travÃ©s del tiempo.</p>"
    },
    {
      "type": "problem",
      "question": "Â¿Por quÃ© las RNNs fallan en capturar dependencias a muy largo plazo?",
      "options": [
        "Por el problema de vanishing gradients: âˆ‚L/âˆ‚h_t â‰ˆ âˆ_{i=1}^t W_{rec} â†’ 0",
        "Debido a limitaciones de memoria hardware",
        "Porque procesan la secuencia al revÃ©s"
      ],
      "answer": "Por el problema de vanishing gradients: âˆ‚L/âˆ‚h_t â‰ˆ âˆ_{i=1}^t W_{rec} â†’ 0",
      "explanation": "ğŸ“‰ <strong>AnÃ¡lisis MatemÃ¡tico:</strong> Si |Î»_max(W_{rec})| < 1, el gradiente decae exponencialmente: ||âˆ‚h_t/âˆ‚h_1|| ~ |Î»|^t â†’ 0 para t grande."
    },
    {
      "type": "text",
      "content": "<h3>ğŸ§® La EcuaciÃ³n Fundamental de la AtenciÃ³n</h3><p>La auto-atenciÃ³n se define como:</p><div class='equation'>\\[ \\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V \\]</div><p><strong>Desglose MatemÃ¡tico:</strong></p><ol><li><em>Producto QKáµ€:</em> Mide compatibilidad entre consultas y claves<br>\\[ \\text{Similitud}(q_i,k_j) = q_i \\cdot k_j \\]</li><li><em>Escalado por 1/âˆšd_k:</em><br>\\[ \\frac{QK^\\top}{\\sqrt{d_k}} \\quad \\text{(Control de varianza)} \\]<p>Si \\( q_i,k_j \\sim \\mathcal{N}(0,1) \\), entonces \\( \\text{Var}(q_i \\cdot k_j) = d_k \\).<br>El escalado mantiene \\( \\text{Var} = 1 \\), evitando saturaciÃ³n del softmax.</p></li><li><em>Softmax:</em> Normaliza a distribuciÃ³n de probabilidad<br>\\[ \\alpha_{ij} = \\frac{e^{s_{ij}}}{\\sum_k e^{s_{ik}}} \\]</li><li><em>CombinaciÃ³n Lineal:</em> Promedia valores segÃºn importancia<br>\\[ h_i = \\sum_j \\alpha_{ij}v_j \\]</li></ol>"
    },
    {
      "type": "problem",
      "question": "Â¿QuÃ© pasarÃ­a si eliminÃ¡ramos el factor de escalado 1/âˆšd_k?",
      "options": [
        "Los logits crecerÃ­an con d_k â†’ softmax saturado â†’ gradientes pequeÃ±os",
        "MejorarÃ­a la eficiencia computacional",
        "No afectarÃ­a el rendimiento"
      ],
      "answer": "Los logits crecerÃ­an con d_k â†’ softmax saturado â†’ gradientes pequeÃ±os",
      "explanation": "ğŸ“š <strong>DemostraciÃ³n:</strong> Para d_k=64, E[QKáµ€] = 64 â†’ softmax([64, 64,...]) â‰ˆ [1/n, ..., 1/n] (gradientes â‰ˆ 0)"
    },
    {
      "type": "text",
      "content": "<h3>ğŸ­ Multi-Head Attention: Por quÃ© mÃºltiples cabezas?</h3><p>DefiniciÃ³n:</p>\\[ \\text{MultiHead}(Q,K,V) = \\text{Concat}(head_1,...,head_h)W^O \\]<p>Donde cada cabeza:</p>\\[ head_i = \\text{Attention}(QW_i^Q,KW_i^K,VW_i^V) \\]<p><strong>Propiedades Clave:</strong></p><ul><li>ğŸ”„ Cada cabeza proyecta en distintos subespacios</li><li>ğŸ§  Captura diferentes relaciones sintÃ¡cticas/semÃ¡nticas</li><li>âš¡ Aumenta capacidad sin incrementar demasiado cÃ³mputo</li></ul>"
    },
    {
      "type": "problem",
      "question": "Si usÃ¡ramos solo 1 cabeza de atenciÃ³n con dimensiÃ³n d=512 vs 8 cabezas de d=64:",
      "options": [
        "8 cabezas: Mayor capacidad de representaciÃ³n + paralelismo",
        "1 cabeza: MÃ¡s eficiente y mejor rendimiento",
        "Es indiferente, son equivalentes"
      ],
      "answer": "8 cabezas: Mayor capacidad de representaciÃ³n + paralelismo",
      "explanation": "ğŸ¯ <strong>AnÃ¡lisis:</strong> 8 cabezas permiten:\\n- ParalelizaciÃ³n total\\n- EspecializaciÃ³n en distintos tipos de patrones\\n- DimensiÃ³n total igual (8Ã—64=512)"
    },
    {
      "type": "text",
      "content": "<h3>ğŸ“ Positional Encoding: La GeometrÃ­a del Orden</h3><p>SoluciÃ³n a la permutaciÃ³n-equivariancia de la atenciÃ³n pura:</p>\\[ PE(pos,2i) = \\sin\\left(\\frac{pos}{10000^{2i/d}}\\right) \\]\\[ PE(pos,2i+1) = \\cos\\left(\\frac{pos}{10000^{2i/d}}\\right) \\]<p><strong>Propiedades MatemÃ¡ticas:</strong></p><ul><li>ğŸ“ Representa posiciones relativas mediante combinaciones lineales</li><li>ğŸŒ€ Frecuencias exponencialmente decrecientes capturan escalas mÃºltiples</li><li>ğŸ§© Permite extrapolaciÃ³n a longitudes no vistas</li></ul>"
    },
    {
      "type": "problem",
      "question": "Â¿Por quÃ© usar funciones sinusoidales en lugar de embeddings aprendidos?",
      "options": [
        "Para generalizar a secuencias mÃ¡s largas que las de entrenamiento",
        "Porque son mÃ¡s fÃ¡ciles de optimizar",
        "Para reducir el coste computacional"
      ],
      "answer": "Para generalizar a secuencias mÃ¡s largas que las de entrenamiento",
      "explanation": "ğŸ“ˆ <strong>Ventaja:</strong> Los sinusoides codifican posiciones de forma absoluta y relativa simultÃ¡neamente mediante identidades trigonomÃ©tricas."
    }
  ]
}
