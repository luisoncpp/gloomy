{
  "title": "Transformers: De la Motivación Matemática a la Implementación",
  "steps": [
    {
      "type": "text",
      "content": "<h3>🔍 Crisis de los Modelos Secuenciales (2017)</h3><p>Problemas fundamentales en NLP pre-Transformers:</p><div class='scroll-container'><table class='model-limits'><tr><th></th><th>RNN/LSTM</th><th>CNN</th><th>Transformers</th></tr><tr><td>Long-range dependencies</td><td>🚫 <code>O(n)</code> pasos</td><td>🚫 Ventana fija</td><td>✅ <code>O(1)</code></td></tr><tr><td>Paralelización</td><td>🚫 Secuencial</td><td>✅ Parcial</td><td>✅ Completa</td></tr><tr><td>Coste computacional</td><td><code>O(n)</code></td><td><code>O(k⋅n)</code></td><td><code>O(n<sup>2</sup>)</code></td></tr></table></div><p><strong>Insight clave:</strong> La atención elimina cuellos de botella en el flujo de información a través del tiempo.</p>"
    },
    {
      "type": "problem",
      "question": "¿Por qué las RNNs fallan en capturar dependencias a muy largo plazo?",
      "options": [
        "Por el problema de vanishing gradients: \\(\\frac{\\partial L}{\\partial h_t} \\approx \\prod_{i=1}^t W_{\\text{rec}} \\to 0\\)",
        "Debido a limitaciones de memoria hardware",
        "Porque procesan la secuencia al revés"
      ],
      "answer": "Por el problema de vanishing gradients: \\(\\frac{\\partial L}{\\partial h_t} \\approx \\prod_{i=1}^t W_{\\text{rec}} \\to 0\\)",
      "explanation": "📉 <strong>Análisis Matemático:</strong> Si \\(|\\lambda_{\\text{máx}}(W_{\\text{rec}})| < 1\\), el gradiente decae exponencialmente: \\(\\left\\|\\frac{\\partial h_t}{\\partial h_1}\\right\\| \\sim |\\lambda|^t \\to 0\\) para \\(t\\) grande."
    },
    {
      "type": "text",
      "content": "<h3>🧮 La Ecuación Fundamental de la Atención</h3><p>La auto-atención se define como:</p><div class='equation-block'>\\[\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\right)V\\]</div><p><strong>Desglose Matemático:</strong></p><ol><li><em>Producto \\(QK^{\\top}\\):</em> Mide compatibilidad entre consultas y claves<br>\\[\\text{Similitud}(q_i,k_j) = q_i \\cdot k_j\\]</li><li><em>Escalado por \\(1/\\sqrt{d_k}\\):</em><br>\\[\\frac{QK^{\\top}}{\\sqrt{d_k}} \\quad \\text{(Control de varianza)}\\]<p>Si \\( q_i,k_j \\sim \\mathcal{N}(0,1) \\), entonces \\( \\text{Var}(q_i \\cdot k_j) = d_k \\).<br>El escalado mantiene \\( \\text{Var} = 1 \\), evitando saturación del softmax.</p></li><li><em>Softmax:</em> Normaliza a distribución de probabilidad<br>\\[\\alpha_{ij} = \\frac{e^{s_{ij}}}{\\sum_k e^{s_{ik}}}\\]</li><li><em>Combinación Lineal:</em> Promedia valores según importancia<br>\\[h_i = \\sum_j \\alpha_{ij}v_j\\]</li></ol>"
    },
    {
      "type": "problem",
      "question": "¿Qué pasaría si elimináramos el factor de escalado \\(1/\\sqrt{d_k}\\)?",
      "options": [
        "Los logits crecerían con \\(d_k\\) → softmax saturado → gradientes pequeños",
        "Mejoraría la eficiencia computacional",
        "No afectaría el rendimiento"
      ],
      "answer": "Los logits crecerían con \\(d_k\\) → softmax saturado → gradientes pequeños",
      "explanation": "📚 <strong>Demostración:</strong> Para \\(d_k=64\\), \\(\\mathbb{E}[QK^{\\top}] = 64\\) → softmax([64, 64,...]) ≈ [1/n, ..., 1/n] (gradientes ≈ 0)"
    },
    {
      "type": "text",
      "content": "<h3>🎭 Multi-Head Attention: Por qué múltiples cabezas?</h3><p>Definición:</p><div class='equation-block'>\\[\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^O\\]</div><p>Donde cada cabeza:</p>\\[\\text{head}_i = \\text{Attention}(QW_i^Q,KW_i^K,VW_i^V)\\]<p><strong>Propiedades Clave:</strong></p><ul><li>🔄 Cada cabeza proyecta en distintos subespacios</li><li>🧠 Captura diferentes relaciones sintácticas/semánticas</li><li>⚡ Aumenta capacidad sin incrementar demasiado cómputo</li></ul>"
    },
    {
      "type": "problem",
      "question": "Si usáramos solo 1 cabeza de atención con dimensión \\(d=512\\) vs 8 cabezas de \\(d=64\\):",
      "options": [
        "8 cabezas: Mayor capacidad de representación + paralelismo",
        "1 cabeza: Más eficiente y mejor rendimiento",
        "Es indiferente, son equivalentes"
      ],
      "answer": "8 cabezas: Mayor capacidad de representación + paralelismo",
      "explanation": "🎯 <strong>Análisis:</strong> 8 cabezas permiten:<br>- Paralelización total<br>- Especialización en distintos tipos de patrones<br>- Dimensión total igual (\\(8×64=512\\))"
    },
    {
      "type": "text",
      "content": "<h3>📍 Positional Encoding: La Geometría del Orden</h3><p>Solución a la permutación-equivariancia de la atención pura:</p><div class='equation-block'>\\[PE_{(pos,2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d}}\\right)\\]\\[PE_{(pos,2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d}}\\right)\\]</div><p><strong>Propiedades Matemáticas:</strong></p><ul><li>📐 Representa posiciones relativas mediante combinaciones lineales</li><li>🌀 Frecuencias exponencialmente decrecientes capturan escalas múltiples</li><li>🧩 Permite extrapolación a longitudes no vistas</li></ul>"
    },
    {
      "type": "problem",
      "question": "¿Por qué usar funciones sinusoidales en lugar de embeddings aprendidos?",
      "options": [
        "Para generalizar a secuencias más largas que las de entrenamiento",
        "Porque son más fáciles de optimizar",
        "Para reducir el coste computacional"
      ],
      "answer": "Para generalizar a secuencias más largas que las de entrenamiento",
      "explanation": "📈 <strong>Ventaja:</strong> Los sinusoides codifican posiciones de forma absoluta y relativa simultáneamente mediante identidades trigonométricas."
    }
  ]
}
