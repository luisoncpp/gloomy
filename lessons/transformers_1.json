{
  "title": "Transformers: De la MotivaciÃ³n MatemÃ¡tica a la ImplementaciÃ³n",
  "steps": [
    {
      "type": "text",
      "content": "<h3>ğŸ” Crisis de los Modelos Secuenciales (2017)</h3><p>Problemas fundamentales en NLP pre-Transformers:</p><div class='scroll-container'><table class='model-limits'><tr><th></th><th>RNN/LSTM</th><th>CNN</th><th>Transformers</th></tr><tr><td>Long-range dependencies</td><td>ğŸš« <code>O(n)</code> pasos</td><td>ğŸš« Ventana fija</td><td>âœ… <code>O(1)</code></td></tr><tr><td>ParalelizaciÃ³n</td><td>ğŸš« Secuencial</td><td>âœ… Parcial</td><td>âœ… Completa</td></tr><tr><td>Coste computacional</td><td><code>O(n)</code></td><td><code>O(kâ‹…n)</code></td><td><code>O(n<sup>2</sup>)</code></td></tr></table></div><p><strong>Insight clave:</strong> La atenciÃ³n elimina cuellos de botella en el flujo de informaciÃ³n a travÃ©s del tiempo.</p>"
    },
    {
      "type": "problem",
      "question": "Â¿Por quÃ© las RNNs fallan en capturar dependencias a muy largo plazo?",
      "options": [
        "Por el problema de vanishing gradients: \\(\\frac{\\partial L}{\\partial h_t} \\approx \\prod_{i=1}^t W_{\\text{rec}} \\to 0\\)",
        "Debido a limitaciones de memoria hardware",
        "Porque procesan la secuencia al revÃ©s"
      ],
      "answer": "Por el problema de vanishing gradients: \\(\\frac{\\partial L}{\\partial h_t} \\approx \\prod_{i=1}^t W_{\\text{rec}} \\to 0\\)",
      "explanation": "ğŸ“‰ <strong>AnÃ¡lisis MatemÃ¡tico:</strong> Si \\(|\\lambda_{\\text{mÃ¡x}}(W_{\\text{rec}})| < 1\\), el gradiente decae exponencialmente: \\(\\left\\|\\frac{\\partial h_t}{\\partial h_1}\\right\\| \\sim |\\lambda|^t \\to 0\\) para \\(t\\) grande."
    },
    {
      "type": "text",
      "content": "<h3>ğŸ§® La EcuaciÃ³n Fundamental de la AtenciÃ³n</h3><p>La auto-atenciÃ³n se define como:</p><div class='equation-block'>\\[\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\right)V\\]</div><p><strong>Desglose MatemÃ¡tico:</strong></p><ol><li><em>Producto \\(QK^{\\top}\\):</em> Mide compatibilidad entre consultas y claves<br>\\[\\text{Similitud}(q_i,k_j) = q_i \\cdot k_j\\]</li><li><em>Escalado por \\(1/\\sqrt{d_k}\\):</em><br>\\[\\frac{QK^{\\top}}{\\sqrt{d_k}} \\quad \\text{(Control de varianza)}\\]<p>Si \\( q_i,k_j \\sim \\mathcal{N}(0,1) \\), entonces \\( \\text{Var}(q_i \\cdot k_j) = d_k \\).<br>El escalado mantiene \\( \\text{Var} = 1 \\), evitando saturaciÃ³n del softmax.</p></li><li><em>Softmax:</em> Normaliza a distribuciÃ³n de probabilidad<br>\\[\\alpha_{ij} = \\frac{e^{s_{ij}}}{\\sum_k e^{s_{ik}}}\\]</li><li><em>CombinaciÃ³n Lineal:</em> Promedia valores segÃºn importancia<br>\\[h_i = \\sum_j \\alpha_{ij}v_j\\]</li></ol>"
    },
    {
      "type": "problem",
      "question": "Â¿QuÃ© pasarÃ­a si eliminÃ¡ramos el factor de escalado \\(1/\\sqrt{d_k}\\)?",
      "options": [
        "Los logits crecerÃ­an con \\(d_k\\) â†’ softmax saturado â†’ gradientes pequeÃ±os",
        "MejorarÃ­a la eficiencia computacional",
        "No afectarÃ­a el rendimiento"
      ],
      "answer": "Los logits crecerÃ­an con \\(d_k\\) â†’ softmax saturado â†’ gradientes pequeÃ±os",
      "explanation": "ğŸ“š <strong>DemostraciÃ³n:</strong> Para \\(d_k=64\\), \\(\\mathbb{E}[QK^{\\top}] = 64\\) â†’ softmax([64, 64,...]) â‰ˆ [1/n, ..., 1/n] (gradientes â‰ˆ 0)"
    },
    {
      "type": "text",
      "content": "<h3>ğŸ­ Multi-Head Attention: Por quÃ© mÃºltiples cabezas?</h3><p>DefiniciÃ³n:</p><div class='equation-block'>\\[\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^O\\]</div><p>Donde cada cabeza:</p>\\[\\text{head}_i = \\text{Attention}(QW_i^Q,KW_i^K,VW_i^V)\\]<p><strong>Propiedades Clave:</strong></p><ul><li>ğŸ”„ Cada cabeza proyecta en distintos subespacios</li><li>ğŸ§  Captura diferentes relaciones sintÃ¡cticas/semÃ¡nticas</li><li>âš¡ Aumenta capacidad sin incrementar demasiado cÃ³mputo</li></ul>"
    },
    {
      "type": "problem",
      "question": "Si usÃ¡ramos solo 1 cabeza de atenciÃ³n con dimensiÃ³n \\(d=512\\) vs 8 cabezas de \\(d=64\\):",
      "options": [
        "8 cabezas: Mayor capacidad de representaciÃ³n + paralelismo",
        "1 cabeza: MÃ¡s eficiente y mejor rendimiento",
        "Es indiferente, son equivalentes"
      ],
      "answer": "8 cabezas: Mayor capacidad de representaciÃ³n + paralelismo",
      "explanation": "ğŸ¯ <strong>AnÃ¡lisis:</strong> 8 cabezas permiten:<br>- ParalelizaciÃ³n total<br>- EspecializaciÃ³n en distintos tipos de patrones<br>- DimensiÃ³n total igual (\\(8Ã—64=512\\))"
    },
    {
      "type": "text",
      "content": "<h3>ğŸ“ Positional Encoding: La GeometrÃ­a del Orden</h3><p>SoluciÃ³n a la permutaciÃ³n-equivariancia de la atenciÃ³n pura:</p><div class='equation-block'>\\[PE_{(pos,2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d}}\\right)\\]\\[PE_{(pos,2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d}}\\right)\\]</div><p><strong>Propiedades MatemÃ¡ticas:</strong></p><ul><li>ğŸ“ Representa posiciones relativas mediante combinaciones lineales</li><li>ğŸŒ€ Frecuencias exponencialmente decrecientes capturan escalas mÃºltiples</li><li>ğŸ§© Permite extrapolaciÃ³n a longitudes no vistas</li></ul>"
    },
    {
      "type": "problem",
      "question": "Â¿Por quÃ© usar funciones sinusoidales en lugar de embeddings aprendidos?",
      "options": [
        "Para generalizar a secuencias mÃ¡s largas que las de entrenamiento",
        "Porque son mÃ¡s fÃ¡ciles de optimizar",
        "Para reducir el coste computacional"
      ],
      "answer": "Para generalizar a secuencias mÃ¡s largas que las de entrenamiento",
      "explanation": "ğŸ“ˆ <strong>Ventaja:</strong> Los sinusoides codifican posiciones de forma absoluta y relativa simultÃ¡neamente mediante identidades trigonomÃ©tricas."
    }
  ]
}
